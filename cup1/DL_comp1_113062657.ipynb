{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./dataset/train.csv')\n",
    "df_test = pd.read_csv('./dataset/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "\n",
    "    \n",
    "    title = soup.body.h1.string.strip().lower()\n",
    "\n",
    "    \n",
    "    article_info = soup.head.find('div', {'class': 'article-info'})\n",
    "    author_name = article_info.find('span', {'class': 'author_name'})\n",
    "    if author_name is not None:\n",
    "        author = author_name.get_text()\n",
    "    elif article_info.span is not None:\n",
    "        author = article_info.span.string\n",
    "    else:\n",
    "        author = article_info.a.string\n",
    "\n",
    "    \n",
    "    author = re.sub(r'\\s+', '_', author.strip().lower().replace(' and ', ' & '))\n",
    "    if author.startswith('by_'):\n",
    "        author = author[3:]\n",
    "\n",
    "    \n",
    "    a_list = soup.body.find('footer', {'class': 'article-topics'}).find_all('a')\n",
    "    topic_list = [a.string.strip().lower() for a in a_list]\n",
    "    topic = ' '.join([re.sub(r'\\s+', '_', t) for t in topic_list])\n",
    "\n",
    "    \n",
    "    try:\n",
    "        date_time = article_info.time['datetime']\n",
    "    except:\n",
    "        date_time = 'Wed, 19 Oct 2024 15:00:00'\n",
    "        \n",
    "    day_map = {'mon': 1, 'tue': 2, 'wed': 3,\n",
    "           'thu': 4, 'fri': 5, 'sat': 6, 'sun': 7}\n",
    "    month_map = {'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6,\n",
    "             'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12}\n",
    "    \n",
    "    match_obj = re.search(r'([\\w]+),\\s+([\\d]+)\\s+([\\w]+)\\s+([\\d]+)\\s+([\\d]+):([\\d]+):([\\d]+)', date_time)\n",
    "\n",
    "    day , date , month , hour = day_map[match_obj.group(1).lower()],  int(match_obj.group(2)) , month_map[match_obj.group(3).lower()], int(match_obj.group(5))\n",
    "    # find content\n",
    "    content = soup.body.find('section', {'class': 'article-content'}).get_text()\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, content)\n",
    "    content = re.sub(r, '', content)\n",
    "    content = re.sub('[\\W]+', ' ', content.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    content_len = len(content)\n",
    "    \n",
    "    return title, author, topic, day, date ,month, hour, content_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list_train = []\n",
    "feature_list_test = []\n",
    "\n",
    "for text in df_train['Page content']:\n",
    "    feature_list_train.append(preprocessor(text))\n",
    "\n",
    "for text in df_test['Page content']:\n",
    "    feature_list_test.append(preprocessor(text))\n",
    "\n",
    "df_parse_train = pd.DataFrame(\n",
    "    feature_list_train,\n",
    "    columns=['Title', 'Author', 'Topic', 'Day', 'Date', 'Month',\n",
    "             'Hour', 'content length']\n",
    ")\n",
    "df_parse_test = pd.DataFrame(\n",
    "    feature_list_test,\n",
    "    columns=['Title', 'Author', 'Topic', 'Day', 'Date', 'Month',\n",
    "             'Hour', 'content length']\n",
    ")\n",
    "columns_to_convert = ['Day', 'Date', 'Month', 'Hour', 'content length']\n",
    "df_parse_train[columns_to_convert] = df_parse_train[columns_to_convert].apply(lambda x: x.astype(np.float32))\n",
    "df_parse_test[columns_to_convert] = df_parse_test[columns_to_convert].apply(lambda x: x.astype(np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]\n",
    "\n",
    "count = CountVectorizer(ngram_range=(1, 1) , tokenizer=tokenizer_stem_nostop, lowercase=False)\n",
    "trans = ColumnTransformer([\n",
    "        ('Title', count, 'Title'),\n",
    "        ('Author',count, 'Author'),\n",
    "        ('Topic', count, 'Topic')],\n",
    "        n_jobs = -1,\n",
    "        remainder='passthrough'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw = pd.DataFrame(df_parse_train , columns = df_parse_train.columns)\n",
    "y_train_raw = (df_train['Popularity'].values == 1).astype(int)\n",
    "X_test = pd.DataFrame(df_parse_test , columns = df_parse_test.columns)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_raw, y_train_raw, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = Pipeline([('ColumnTrans', trans),\n",
    "                 ('Classifier', LGBMClassifier(random_state = 0, learning_rate = 0.007, n_estimators = 400))])\n",
    "forest = Pipeline([('ColumnTrans', trans),\n",
    "                   ('Classifier', RandomForestClassifier(n_jobs = -1, random_state = 0, n_estimators = 300))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 10885, number of negative: 11229\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047744 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4618\n",
      "[LightGBM] [Info] Number of data points in the train set: 22114, number of used features: 2021\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492222 -> initscore=-0.031114\n",
      "[LightGBM] [Info] Start training from score -0.031114\n",
      "train auc: 0.95906\n",
      "valid auc: 0.58676\n"
     ]
    }
   ],
   "source": [
    "voting = VotingClassifier([('lgbm', lgbm), ('forest', forest)],\n",
    "                          voting='soft', weights=[1, 0.25])\n",
    "voting.fit(X_train , y_train)\n",
    "print('train auc: %.5f' % roc_auc_score(y_train, voting.predict_proba(X_train)[:, 1]))\n",
    "print('valid auc: %.5f' % roc_auc_score(y_valid, voting.predict_proba(X_valid)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = voting.predict_proba(X_test)[:, 1]\n",
    "df_pred = pd.DataFrame({'Id': df_test['Id'], 'Popularity': y_score})\n",
    "df_pred.to_csv('test_pred.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report\n",
    "\n",
    "**Student ID**: 113062657  \n",
    "**Name**: 黃盛揚\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Data Preprocessing\n",
    "\n",
    "\n",
    "在Preprocess中，我使用 **BeautifulSoup** 來拆解每篇文章的 HTML 結構，提取了 **Title**、**Author**、**Topic**、**Date** 和 **Content Length** 等欄位。原本曾考慮使用**Content**作為特徵，但因為記憶體使用過高且預測效果不佳，最終僅提取 **Content Length** 作為特徵，以減少記憶體消耗並提升模型效率。對於文字型欄位（**Title**、**Author** 和 **Topic**），我使用了 **Bag-of-Words (BOW)** 模型來將其轉換為數值特徵。使用 **CountVectorizer** 進行特徵向量化，並搭配自訂的詞幹分析器（stemming）來減少詞彙維度，過濾掉 **stopwords**，以強調主要資訊並降低模型的負擔。在資料分割方面，我將訓練資料集分為訓練集與驗證集，比例為 80:20。\n",
    "\n",
    "### 2. Model Building\n",
    "\n",
    "為了提升預測準確性與穩定性，我選擇了 **Voting Classifier** 作為主模型，並結合了 **LightGBM** 和 **Random Forest** 的優勢：\n",
    "- **LightGBM** 有高效能和處理大量數據的優點。\n",
    "- **Random Forest** 比起前者增加了模型的多樣性與穩定性。\n",
    "\n",
    "最後對 Voting Classifier 使用「soft voting」策略，將各模型的預測機率進行加權平均，以達到更精確的分類結果。\n",
    "模型的訓練過程中，我首先使用 **Pipeline** 將特徵轉換與分類模型整合，並且對 **LightGBM** 使用低learning rate和適量的n_estimators來控制模型的學習深度，確保模型訓練的穩定性。對於 **Random Forest**，則設置較高的樹數以增強模型穩定性，並減少overfittingS的風險。\n",
    "\n",
    "### 3. Takeaways\n",
    "\n",
    "- 使用簡化特徵（例如 **Content Length** 取代全文內容）大幅減少了記憶體需求，且能維持合理的分類表現。\n",
    "- Voting Classifier 組合了 **LightGBM** 和 **Random Forest** 的優勢，提供更快的預測速度和更高的穩定性。\n",
    "- 記憶體的限制使得無法處理完整的文章文本，突顯了特徵選擇與簡化的重要性。\n",
    "- 不同模型的組合與權重選擇需多次測試才能達到最優結果。\n",
    "\n",
    "### Lessons Learned\n",
    "preprocess很影響這次競賽的結果，在挑選feature的時候也遇到了不少的問題，原本以為很重要的content不僅很難處理，score還特別差，但我認為更重要的還是運氣。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
